{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4d4b9e0",
   "metadata": {},
   "source": [
    "# Federated Learning Model Evaluation Across Data Partitions\n",
    "\n",
    "This notebook evaluates BiSeNetV2 models trained with different federated learning aggregators (FedAvg, FedProx) across multiple communication rounds and client data partitions. It calculates mIoU and F1-score metrics for each client and visualizes the progression of these metrics across communication round batches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9457e3",
   "metadata": {},
   "source": [
    "## Section 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c184c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.insert(0, '/home/moustafa/Me/Projects/Grad/Code/BiseNet-FL/fl-cityscapes-bisenetv2/')\n",
    "\n",
    "# Import from the project\n",
    "from lib.models import BiSeNetV2\n",
    "from lib.data import get_data_loader\n",
    "import lib.data.transform_cv2 as T\n",
    "from fl_cityscapes_bisenetv2.data_preparation.client_dataset import CityScapesClientDataset\n",
    "from tools.eval_metrics import compute_metrics_from_cm\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf328e1",
   "metadata": {},
   "source": [
    "## Section 2: Define Model Loading and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3030c9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loading and evaluation functions defined successfully!\n"
     ]
    }
   ],
   "source": [
    "class MetricsCalculator:\n",
    "    \"\"\"Calculate semantic segmentation metrics from predictions and labels.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_classes, ignore_label=255):\n",
    "        self.n_classes = n_classes\n",
    "        self.ignore_label = ignore_label\n",
    "        self.confusion_matrix = np.zeros((n_classes, n_classes))\n",
    "    \n",
    "    def update(self, predictions, labels):\n",
    "        \"\"\"Update confusion matrix with new predictions and labels.\"\"\"\n",
    "        # Ensure inputs are numpy arrays\n",
    "        predictions = predictions.cpu().numpy() if isinstance(predictions, torch.Tensor) else predictions\n",
    "        labels = labels.cpu().numpy() if isinstance(labels, torch.Tensor) else labels\n",
    "        \n",
    "        # Flatten and remove ignore label\n",
    "        predictions = predictions.flatten()\n",
    "        labels = labels.flatten()\n",
    "        \n",
    "        mask = labels != self.ignore_label\n",
    "        predictions = predictions[mask]\n",
    "        labels = labels[mask]\n",
    "        \n",
    "        # Update confusion matrix\n",
    "        np.add.at(\n",
    "            self.confusion_matrix,\n",
    "            (labels, predictions),\n",
    "            1\n",
    "        )\n",
    "    \n",
    "    def compute_metrics(self) -> Dict[str, float]:\n",
    "        \"\"\"Compute mIoU and F1-score from confusion matrix.\"\"\"\n",
    "        return compute_metrics_from_cm(self.confusion_matrix)\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset confusion matrix.\"\"\"\n",
    "        self.confusion_matrix = np.zeros((self.n_classes, self.n_classes))\n",
    "\n",
    "\n",
    "def load_model(model_path: str, n_classes: int = 19, device: str = 'cuda') -> BiSeNetV2:\n",
    "    \"\"\"Load BiSeNetV2 model from checkpoint.\"\"\"\n",
    "    model = BiSeNetV2(n_classes, aux_mode='eval')\n",
    "    \n",
    "    if os.path.exists(model_path):\n",
    "        checkpoint = torch.load(model_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint, strict=False)\n",
    "        print(f\"✓ Model loaded from {model_path}\")\n",
    "    else:\n",
    "        print(f\"✗ Model path not found: {model_path}\")\n",
    "        return None\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_client_dataset(\n",
    "    im_root: str,\n",
    "    partition_file: str,\n",
    "    partition_id: str,\n",
    "    n_classes: int = 19,\n",
    "    batch_size: int = 4\n",
    "):\n",
    "    \"\"\"Load client dataset for evaluation.\"\"\"\n",
    "    try:\n",
    "        with open(partition_file, 'r', encoding='utf-8') as f:\n",
    "            data_partitions = json.load(f)\n",
    "        \n",
    "        partition = data_partitions[str(partition_id)]\n",
    "        \n",
    "        # Get normalization metrics for this partition\n",
    "        normalization_metrics = partition.get(\"data_metrics\", {})\n",
    "        \n",
    "        ds = CityScapesClientDataset(\n",
    "            im_root,\n",
    "            partition[\"data\"],\n",
    "            normalization_metrics,\n",
    "            T.TransformationVal()\n",
    "        )\n",
    "        \n",
    "        from torch.utils.data import DataLoader\n",
    "        dataloader = DataLoader(\n",
    "            ds,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=2,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        return dataloader, len(ds)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading client {partition_id} data: {e}\")\n",
    "        return None, 0\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_model_on_client(\n",
    "    model: BiSeNetV2,\n",
    "    dataloader,\n",
    "    n_classes: int = 19,\n",
    "    device: str = 'cuda',\n",
    "    return_confusion_matrix: bool = False\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Evaluate model on client dataset.\n",
    "    \n",
    "    Args:\n",
    "        return_confusion_matrix: If True, return both metrics and confusion matrix.\n",
    "                                Returns (metrics_dict, confusion_matrix) tuple.\n",
    "                                If False, returns only metrics_dict.\n",
    "    \"\"\"\n",
    "    if model is None or dataloader is None:\n",
    "        return None\n",
    "    \n",
    "    metrics_calc = MetricsCalculator(n_classes, ignore_label=255)\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            logits = outputs[0]  # BiSeNetV2 returns (main_output, aux_outputs...)\n",
    "            \n",
    "            # Get predictions\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            # Update metrics\n",
    "            metrics_calc.update(predictions, labels.squeeze(1))\n",
    "    \n",
    "    metrics = metrics_calc.compute_metrics()\n",
    "    \n",
    "    if return_confusion_matrix:\n",
    "        return metrics, metrics_calc.confusion_matrix\n",
    "    return metrics\n",
    "\n",
    "print(\"Model loading and evaluation functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e066d06",
   "metadata": {},
   "source": [
    "## Section 3: Set Up Directory Paths and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc69ca06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "============================================================\n",
      "Path Configuration\n",
      "============================================================\n",
      "Project Root: /home/moustafa/Me/Projects/Grad/Code/BiseNet-FL\n",
      "Results Root: /home/moustafa/Me/Projects/Grad/Experiments/City_Partitions\n",
      "Data Root: /home/moustafa/Me/Projects/Grad/Code/BiseNet-FL/datasets/cityscapes\n",
      "Data Partition File: /home/moustafa/Me/Projects/Grad/Code/BiseNet-FL/datasets/cityscapes/city_partitions.json\n",
      "Aggregators: ['FedAvg', 'FedProx']\n",
      "\n",
      "============================================================\n",
      "Discovering Available Models\n",
      "============================================================\n",
      "\n",
      "✓ Found models at: /home/moustafa/Me/Projects/Grad/Experiments/City_Partitions/FedAvg/new\n",
      "  └─ Round 120: ✓ latest_model.pt found\n",
      "  └─ Round 180: ✓ latest_model.pt found\n",
      "  └─ Round 240: ✓ latest_model.pt found\n",
      "  └─ Round 300: ✓ latest_model.pt found\n",
      "  └─ Round 360: ✓ latest_model.pt found\n",
      "  └─ Round 420: ✓ latest_model.pt found\n",
      "  └─ Round 480: ✓ latest_model.pt found\n",
      "  └─ Round 540: ✓ latest_model.pt found\n",
      "  └─ Round 60: ✓ latest_model.pt found\n",
      "\n",
      "✓ Found models at: /home/moustafa/Me/Projects/Grad/Experiments/City_Partitions/FedProx/new\n",
      "  └─ Round 120: ✓ latest_model.pt found\n",
      "  └─ Round 180: ✓ latest_model.pt found\n",
      "  └─ Round 240: ✓ latest_model.pt found\n",
      "  └─ Round 300: ✓ latest_model.pt found\n",
      "  └─ Round 360: ✓ latest_model.pt found\n",
      "  └─ Round 420: ✓ latest_model.pt found\n",
      "  └─ Round 480: ✓ latest_model.pt found\n",
      "  └─ Round 60: ✓ latest_model.pt found\n",
      "\n",
      "============================================================\n",
      "Configuration Complete. Ready for model evaluation.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION - Modify these paths according to your directory structure\n",
    "# ============================================================================\n",
    "\n",
    "PROJECT_ROOT = Path('/home/moustafa/Me/Projects/Grad/Code/BiseNet-FL')\n",
    "RESULTS_ROOT = Path('/home/moustafa/Me/Projects/Grad/Experiments/City_Partitions')  # Adjust as needed\n",
    "DATA_ROOT = PROJECT_ROOT / 'datasets' / 'cityscapes'\n",
    "\n",
    "\n",
    "PARTITION_ID = \"City_Partitions\"\n",
    "PARTITION_CLIENTS = 18\n",
    "\n",
    "# Data partition configuration\n",
    "DATA_PARTITION_FILE = DATA_ROOT / 'city_partitions.json'\n",
    "NUM_CLASSES = 19  # Cityscapes has 19 semantic classes\n",
    "\n",
    "# Aggregators to evaluate\n",
    "AGGREGATORS = ['FedAvg', 'FedProx']  # Add more aggregators if available\n",
    "\n",
    "# Device configuration\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Batch size for evaluation\n",
    "EVAL_BATCH_SIZE = 4\n",
    "\n",
    "# ============================================================================\n",
    "# Verify Paths and List Available Models\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Path Configuration\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Project Root: {PROJECT_ROOT}\")\n",
    "print(f\"Results Root: {RESULTS_ROOT}\")\n",
    "print(f\"Data Root: {DATA_ROOT}\")\n",
    "print(f\"Data Partition File: {DATA_PARTITION_FILE}\")\n",
    "# print(f\"Number of Partitions: {NUM_PARTITIONS}\")\n",
    "print(f\"Aggregators: {AGGREGATORS}\")\n",
    "# print(f\"Communication Round Batches: {COMMUNICATION_ROUND_BATCHES}\")\n",
    "\n",
    "# List available models\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Discovering Available Models\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "available_models = {}\n",
    "    \n",
    "for aggregator in AGGREGATORS:\n",
    "    aggregator_key = aggregator\n",
    "    available_models[aggregator_key] = []\n",
    "        \n",
    "    # Try to find model directories\n",
    "    base_path = RESULTS_ROOT / aggregator / 'new'\n",
    "        \n",
    "    if base_path.exists():\n",
    "        print(f\"\\n✓ Found models at: {base_path}\")\n",
    "        # List subdirectories (communication round batches)\n",
    "        for item in sorted(base_path.iterdir()):\n",
    "            if item.is_dir() and item.name.isdigit():\n",
    "                round_num = int(item.name)\n",
    "                latest_model = item / 'latest_model.pt'\n",
    "                if latest_model.exists():\n",
    "                    available_models[aggregator_key].append(round_num)\n",
    "                    print(f\"  └─ Round {round_num}: ✓ latest_model.pt found\")\n",
    "                else:\n",
    "                    print(f\"  └─ Round {round_num}: ✗ latest_model.pt NOT found\")\n",
    "    else:\n",
    "        print(f\"\\n✗ No models found for aggregator {aggregator} at expected path: {base_path}\")\n",
    "\n",
    "    if not available_models[aggregator_key]:\n",
    "        print(f\"  ✗ No models found for aggregator {aggregator}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Configuration Complete. Ready for model evaluation.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba4fd97",
   "metadata": {},
   "source": [
    "## Section 4: Load Models and Evaluate Per Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83e32ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STARTING MODEL EVALUATION ACROSS ALL CONFIGURATIONS\n",
      "======================================================================\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Evaluating: Partition City_Partitions | Aggregator: FedAvg\n",
      "Base Path: /home/moustafa/Me/Projects/Grad/Experiments/City_Partitions/FedAvg/new\n",
      "----------------------------------------------------------------------\n",
      "Found rounds: [60, 120, 180, 240, 300, 360, 420, 480, 540]\n",
      "\n",
      "  Loading model for round 60...\n",
      "✓ Model loaded from /home/moustafa/Me/Projects/Grad/Experiments/City_Partitions/FedAvg/new/60/latest_model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Client 0: mIoU=0.3952, F1=0.4637 (174 samples)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Client 1: mIoU=0.3904, F1=0.4637 (96 samples)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Client 2: mIoU=0.4043, F1=0.4740 (316 samples)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Client 3: mIoU=0.3916, F1=0.4626 (154 samples)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Client 4: mIoU=0.3519, F1=0.4188 (85 samples)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Client 5: mIoU=0.3803, F1=0.4481 (221 samples)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Client 6: mIoU=0.4146, F1=0.4839 (109 samples)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Client 7: mIoU=0.3823, F1=0.4551 (248 samples)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Client 8: mIoU=0.4001, F1=0.4670 (196 samples)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Client 9: mIoU=0.4020, F1=0.4734 (119 samples)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Client 10: mIoU=0.3826, F1=0.4465 (99 samples)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Client 11: mIoU=0.3742, F1=0.4471 (94 samples)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Client 12: mIoU=0.3859, F1=0.4573 (365 samples)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Client 13: mIoU=0.3755, F1=0.4442 (196 samples)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Client 14: mIoU=0.3809, F1=0.4555 (144 samples)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Client 15: mIoU=0.3953, F1=0.4691 (95 samples)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Client 16: mIoU=0.4025, F1=0.4747 (142 samples)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Client 17: mIoU=0.3785, F1=0.4563 (122 samples)\n",
      "\n",
      "  Loading model for round 120...\n",
      "✓ Model loaded from /home/moustafa/Me/Projects/Grad/Experiments/City_Partitions/FedAvg/new/120/latest_model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Client 0: mIoU=0.4833, F1=0.5704 (174 samples)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Client 1: mIoU=0.4854, F1=0.5775 (96 samples)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Client 2: mIoU=0.5182, F1=0.6046 (316 samples)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Client 3: mIoU=0.4695, F1=0.5469 (154 samples)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Client 4: mIoU=0.4453, F1=0.5324 (85 samples)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Client 5: mIoU=0.4677, F1=0.5465 (221 samples)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Client 6: mIoU=0.5091, F1=0.5885 (109 samples)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Client 7: mIoU=0.4900, F1=0.5849 (248 samples)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Client 8: mIoU=0.4985, F1=0.5821 (196 samples)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Evaluation Loop: Iterate through all configurations\n",
    "# ============================================================================\n",
    "\n",
    "# Storage for evaluation results (includes confusion matrices for per-class analysis)\n",
    "evaluation_results = defaultdict(lambda: defaultdict(lambda: defaultdict(dict)))\n",
    "\n",
    "def find_model_base_path(aggregator: str) -> Optional[Path]:\n",
    "    \"\"\"Find the base path for models of a specific partition and aggregator.\"\"\"\n",
    "    possible_path = RESULTS_ROOT / aggregator / 'new'\n",
    "    \n",
    "    if possible_path.exists():\n",
    "        return possible_path\n",
    "    return None\n",
    "\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"STARTING MODEL EVALUATION ACROSS ALL CONFIGURATIONS\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Track which models were actually evaluated\n",
    "evaluated_configs = []\n",
    "\n",
    "for aggregator in AGGREGATORS:\n",
    "    base_path = find_model_base_path(aggregator)\n",
    "    \n",
    "    if base_path is None:\n",
    "        print(f\"\\n[ERROR] Partition {PARTITION_ID}, {aggregator}: No model directory found\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n{'-'*70}\")\n",
    "    print(f\"Evaluating: Partition {PARTITION_ID} | Aggregator: {aggregator}\")\n",
    "    print(f\"Base Path: {base_path}\")\n",
    "    print(f\"{'-'*70}\")\n",
    "    \n",
    "    # Get available communication round directories\n",
    "    round_dirs = sorted([\n",
    "        int(d.name) for d in base_path.iterdir() \n",
    "        if d.is_dir() and d.name.isdigit()\n",
    "    ])\n",
    "    \n",
    "    if not round_dirs:\n",
    "        print(f\"✗ No communication round directories found\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"Found rounds: {round_dirs}\")\n",
    "    \n",
    "    for comm_round in round_dirs:\n",
    "        round_path = base_path / str(comm_round)\n",
    "        latest_model_path = round_path / 'latest_model.pt'\n",
    "\n",
    "        if not latest_model_path.exists():\n",
    "            print(f\"  ✗ Round {comm_round}: latest_model.pt not found\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n  Loading model for round {comm_round}...\")\n",
    "        model = load_model(str(latest_model_path), n_classes=NUM_CLASSES, device=DEVICE)\n",
    "        \n",
    "        if model is None:\n",
    "            continue\n",
    "        \n",
    "        # Evaluate on each client\n",
    "        for client_id in range(PARTITION_CLIENTS):\n",
    "            try:\n",
    "                # Load client dataset\n",
    "                dataloader, num_samples = load_client_dataset(\n",
    "                    str(DATA_ROOT),\n",
    "                    str(DATA_PARTITION_FILE),\n",
    "                    client_id,\n",
    "                    n_classes=NUM_CLASSES,\n",
    "                    batch_size=EVAL_BATCH_SIZE\n",
    "                )\n",
    "                \n",
    "                if dataloader is None:\n",
    "                    continue\n",
    "                \n",
    "                # Evaluate model on client data (collect confusion matrix for per-class analysis)\n",
    "                metrics, confusion_matrix = evaluate_model_on_client(\n",
    "                    model,\n",
    "                    dataloader,\n",
    "                    n_classes=NUM_CLASSES,\n",
    "                    device=DEVICE,\n",
    "                    return_confusion_matrix=True\n",
    "                )\n",
    "                \n",
    "                if metrics is not None:\n",
    "                    # Store results with confusion matrix for per-class analysis\n",
    "                    evaluation_results[aggregator][client_id][comm_round] = {\n",
    "                        'metrics': metrics,\n",
    "                        'confusion_matrix': confusion_matrix\n",
    "                    }\n",
    "                    \n",
    "                    miou = metrics.get('mIoU', 0.0)\n",
    "                    f1 = metrics.get('F1_Score', 0.0)\n",
    "                    \n",
    "                    print(f\"    Client {client_id}: mIoU={miou:.4f}, F1={f1:.4f} ({num_samples} samples)\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    ✗ Error evaluating client {client_id}: {str(e)}\")\n",
    "        \n",
    "        # Free memory\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Best model evaluation\n",
    "    best_round_path = base_path / str(round_dirs[-1])\n",
    "    best_model_path = best_round_path / 'best_model.pt'\n",
    "    comm_round = \"Best\"\n",
    "    if not best_model_path.exists():\n",
    "        best_model_path = base_path / str(round_dirs[-2]) / 'latest_model.pt'\n",
    "\n",
    "    print(f\"\\n  Loading model for round {comm_round}...\")\n",
    "    model = load_model(str(best_model_path), n_classes=NUM_CLASSES, device=DEVICE)\n",
    "    \n",
    "    # Evaluate on each client\n",
    "    for client_id in range(PARTITION_CLIENTS):\n",
    "        try:\n",
    "            # Load client dataset\n",
    "            dataloader, num_samples = load_client_dataset(\n",
    "                str(DATA_ROOT),\n",
    "                str(DATA_PARTITION_FILE),\n",
    "                client_id,\n",
    "                n_classes=NUM_CLASSES,\n",
    "                batch_size=EVAL_BATCH_SIZE\n",
    "            )\n",
    "            \n",
    "            if dataloader is None:\n",
    "                continue\n",
    "            \n",
    "            # Evaluate model on client data (collect confusion matrix for per-class analysis)\n",
    "            metrics, confusion_matrix = evaluate_model_on_client(\n",
    "                model,\n",
    "                dataloader,\n",
    "                n_classes=NUM_CLASSES,\n",
    "                device=DEVICE,\n",
    "                return_confusion_matrix=True\n",
    "            )\n",
    "            \n",
    "            if metrics is not None:\n",
    "                # Store results with confusion matrix for per-class analysis\n",
    "                evaluation_results[aggregator][client_id][comm_round] = {\n",
    "                    'metrics': metrics,\n",
    "                    'confusion_matrix': confusion_matrix\n",
    "                }\n",
    "                \n",
    "                miou = metrics.get('mIoU', 0.0)\n",
    "                f1 = metrics.get('F1_Score', 0.0)\n",
    "                \n",
    "                print(f\"    Client {client_id}: mIoU={miou:.4f}, F1={f1:.4f} ({num_samples} samples)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    ✗ Error evaluating client {client_id}: {str(e)}\")\n",
    "    \n",
    "    # Free memory\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    evaluated_configs.append((PARTITION_ID, aggregator))\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"EVALUATION COMPLETE\")\n",
    "print(f\"Configurations evaluated: {len(evaluated_configs)}\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca32be1a",
   "metadata": {},
   "source": [
    "## Section 5: Aggregate Metrics Across Communication Rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c18654e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_and_prepare_plot_data(\n",
    "    evaluation_results,\n",
    "    aggregator: str,\n",
    "    metric: str = 'mIoU'\n",
    ") -> Tuple[Dict, Dict]:\n",
    "    \"\"\"\n",
    "    Aggregate evaluation results for plotting.\n",
    "    Handles both old format (direct metrics) and new format (metrics + confusion_matrix).\n",
    "    \n",
    "    Returns:\n",
    "        plot_data: {client_id: {round: metric_value}}\n",
    "    \"\"\"\n",
    "    plot_data = defaultdict(dict)\n",
    "    \n",
    "    if aggregator not in evaluation_results:\n",
    "        return dict(plot_data)\n",
    "    \n",
    "    agg_results = evaluation_results[aggregator]\n",
    "    \n",
    "    for client_id in agg_results:\n",
    "        client_data = agg_results[client_id]\n",
    "        \n",
    "        # Store metric values for each communication round\n",
    "        for comm_round in client_data.keys():\n",
    "            result = client_data[comm_round]\n",
    "            \n",
    "            # Handle new format: {'metrics': {...}, 'confusion_matrix': ...}\n",
    "            if isinstance(result, dict) and 'metrics' in result:\n",
    "                metrics = result['metrics']\n",
    "            else:\n",
    "                # Handle old format: direct metrics dict\n",
    "                metrics = result\n",
    "            \n",
    "            if metric in metrics:\n",
    "                plot_data[client_id][comm_round] = metrics[metric]\n",
    "    \n",
    "    return dict(plot_data)\n",
    "\n",
    "\n",
    "# Process results for each partition and aggregator\n",
    "processed_results = {}\n",
    "\n",
    "for aggregator in evaluation_results:\n",
    "    processed_results[aggregator] = {}\n",
    "    \n",
    "    # Prepare data for both metrics\n",
    "    miou_data = aggregate_and_prepare_plot_data(\n",
    "            evaluation_results, aggregator, metric='mIoU'\n",
    "    )\n",
    "    f1_data = aggregate_and_prepare_plot_data(\n",
    "        evaluation_results, aggregator, metric='F1_Score'\n",
    "    )\n",
    "\n",
    "    processed_results[aggregator] = {\n",
    "        'mIoU': {\n",
    "            'data': miou_data,\n",
    "        },\n",
    "        'F1_Score': {\n",
    "            'data': f1_data,\n",
    "        }\n",
    "    }\n",
    "\n",
    "print(\"Metrics aggregated and prepared for visualization!\")\n",
    "print(f\"\\nProcessed results for {len(processed_results)} partitions\")\n",
    "\n",
    "# Display summary statistics\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"EVALUATION SUMMARY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e633596e",
   "metadata": {},
   "source": [
    "## Section 6: Visualize Client Performance with Bar Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a522da8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_client_metrics(\n",
    "    plot_data: Dict,\n",
    "    metric_name: str = 'mIoU',\n",
    "    aggregator: str = 'FedAvg',\n",
    "    figsize: Tuple[int, int] = (14, 6)\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a grouped bar plot for client performance across communication rounds.\n",
    "    \n",
    "    Args:\n",
    "        plot_data: {client_id: {round: metric_value}}\n",
    "        metric_name: Name of the metric being plotted\n",
    "        aggregator: Name of the aggregator\n",
    "        partition_id: ID of the data partition\n",
    "        figsize: Figure size tuple\n",
    "    \"\"\"\n",
    "    if not plot_data:\n",
    "        print(f\"No data to plot for {aggregator}, Partition {PARTITION_ID}\")\n",
    "        return\n",
    "    \n",
    "    # Prepare data for plotting\n",
    "    client_ids = sorted(plot_data.keys())\n",
    "    all_rounds = [r for client_rounds in plot_data.values() for r in client_rounds.keys()]\n",
    "    \n",
    "    # Create dataframe for easier plotting\n",
    "    plot_df_list = []\n",
    "    for client_id in client_ids:\n",
    "        client_rounds = plot_data[client_id]\n",
    "        for round_num in all_rounds:\n",
    "            value = client_rounds.get(round_num, None)\n",
    "            if value is not None:\n",
    "                plot_df_list.append({\n",
    "                    'Client': f'Client {client_id}',\n",
    "                    'Communication Round': round_num,\n",
    "                    metric_name: value,\n",
    "                    'Model Type': f'Round {round_num}'\n",
    "                })\n",
    "        \n",
    "    if not plot_df_list:\n",
    "        print(f\"No valid data points for plotting\")\n",
    "        return\n",
    "    \n",
    "    plot_df = pd.DataFrame(plot_df_list)\n",
    "    \n",
    "    # Create the plot\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    # Get unique rounds for coloring\n",
    "    rounds_and_best = sorted(\n",
    "        set(plot_df['Model Type']),\n",
    "        key=lambda x: (x == 'Round Best', int(x.split()[-1]) if x != 'Round Best' else float('inf'))\n",
    "    )\n",
    "    colors = plt.cm.tab20(np.linspace(0, 1, len(rounds_and_best)))\n",
    "    color_map = {r: colors[i] for i, r in enumerate(rounds_and_best)}\n",
    "    \n",
    "    # Create grouped bar plot\n",
    "    x = np.arange(len(client_ids))\n",
    "    width = 0.8 / (len(rounds_and_best) + 0.5)\n",
    "    \n",
    "    for idx, model_type in enumerate(rounds_and_best):\n",
    "        data_subset = plot_df[plot_df['Model Type'] == model_type]\n",
    "        values = [\n",
    "            data_subset[data_subset['Client'] == f'Client {cid}'][metric_name].values[0]\n",
    "            if len(data_subset[data_subset['Client'] == f'Client {cid}']) > 0\n",
    "            else 0\n",
    "            for cid in client_ids\n",
    "        ]\n",
    "        \n",
    "        offset = (idx - len(rounds_and_best)/2 + 0.5) * width\n",
    "\n",
    "        # Highlight 'Round Best' bar differently\n",
    "        if model_type == 'Round Best':\n",
    "            ax.bar(x + offset, values, width, label=model_type, \n",
    "                  color=color_map[model_type], edgecolor='black', linewidth=2, alpha=0.9)\n",
    "        else:\n",
    "            ax.bar(x + offset, values, width, label=model_type, \n",
    "                  color=color_map[model_type], alpha=0.7)\n",
    "    \n",
    "    # Customize plot\n",
    "    ax.set_xlabel('Clients', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel(metric_name, fontsize=12, fontweight='bold')\n",
    "    ax.set_title(f'{metric_name} per Client\\nPartition {PARTITION_ID} | Aggregator: {aggregator}', \n",
    "                fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([f'C{cid}' for cid in client_ids])\n",
    "    ax.legend(title='Model', bbox_to_anchor=(1.05, 1), loc='upper left', ncol=1)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    ax.set_ylim([0, max(plot_df[metric_name]) * 1.1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "# Generate plots for each partition and aggregator\n",
    "plots_generated = 0\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Generating plots for Partition {PARTITION_ID}\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "for aggregator in sorted(processed_results.keys()):\n",
    "    # Plot mIoU\n",
    "    miou_data = processed_results[aggregator]['mIoU']['data']\n",
    "\n",
    "    if miou_data:\n",
    "        fig, ax = plot_client_metrics(\n",
    "            miou_data,\n",
    "            metric_name='mIoU',\n",
    "            aggregator=aggregator,\n",
    "        )\n",
    "        plt.savefig(\n",
    "            f'miou_partition_{PARTITION_ID}_{aggregator}.png',\n",
    "            dpi=150,\n",
    "            bbox_inches='tight'\n",
    "        )\n",
    "        plt.show()\n",
    "        plots_generated += 1\n",
    "        print(f\"  ✓ Saved: miou_partition_{PARTITION_ID}_{aggregator}.png\")\n",
    "\n",
    "    # Plot F1-Score\n",
    "    f1_data = processed_results[aggregator]['F1_Score']['data']\n",
    "\n",
    "    if f1_data:\n",
    "        fig, ax = plot_client_metrics(\n",
    "            f1_data,\n",
    "            metric_name='F1-Score',\n",
    "            aggregator=aggregator,\n",
    "        )\n",
    "        plt.savefig(\n",
    "            f'f1score_partition_{PARTITION_ID}_{aggregator}.png',\n",
    "            dpi=150,\n",
    "            bbox_inches='tight'\n",
    "        )\n",
    "        plt.show()\n",
    "        plots_generated += 1\n",
    "        print(f\"  ✓ Saved: f1score_partition_{PARTITION_ID}_{aggregator}.png\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"VISUALIZATION COMPLETE: {plots_generated} plots generated\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21e164d",
   "metadata": {},
   "source": [
    "## Section 7: Variance Analysis - Client Heterogeneity Across Communication Rounds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fff29de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_variance_metrics(evaluation_results):\n",
    "    \"\"\"\n",
    "    Calculate Coefficient of Variation (CoV) of mIoU across clients for each round.\n",
    "    Handles both old format (direct metrics) and new format (metrics + confusion_matrix).\n",
    "    \n",
    "    Returns:\n",
    "        variance_data: {aggregator: {round: {'mean': float, 'std': float, 'cov': float}}}\n",
    "    \"\"\"\n",
    "    variance_data = defaultdict(lambda: defaultdict(dict))\n",
    "    \n",
    "    for aggregator in evaluation_results:\n",
    "        client_results = evaluation_results[aggregator]\n",
    "        \n",
    "        # Get all unique rounds\n",
    "        all_rounds = set()\n",
    "        for client_id in client_results:\n",
    "            all_rounds.update(client_results[client_id].keys())\n",
    "        \n",
    "        # Calculate variance for each round\n",
    "        for comm_round in sorted(all_rounds, key=lambda x: (x == 'Best', int(x) if isinstance(x, int) else float('inf'))):\n",
    "            round_metrics = []\n",
    "            \n",
    "            for client_id in client_results:\n",
    "                if comm_round in client_results[client_id]:\n",
    "                    result = client_results[client_id][comm_round]\n",
    "                    \n",
    "                    # Handle new format: {'metrics': {...}, 'confusion_matrix': ...}\n",
    "                    if isinstance(result, dict) and 'metrics' in result:\n",
    "                        metrics = result['metrics']\n",
    "                    else:\n",
    "                        # Handle old format: direct metrics dict\n",
    "                        metrics = result\n",
    "                    \n",
    "                    miou = metrics.get('mIoU', None)\n",
    "                    if miou is not None:\n",
    "                        round_metrics.append(miou)\n",
    "            \n",
    "            if round_metrics:\n",
    "                mean_miou = np.mean(round_metrics)\n",
    "                std_miou = np.std(round_metrics)\n",
    "                cov = (std_miou / mean_miou * 100) if mean_miou > 0 else 0  # CoV as percentage\n",
    "                \n",
    "                variance_data[aggregator][comm_round] = {\n",
    "                    'mean': mean_miou,\n",
    "                    'std': std_miou,\n",
    "                    'cov': cov,\n",
    "                    'num_clients': len(round_metrics)\n",
    "                }\n",
    "    \n",
    "    return dict(variance_data)\n",
    "\n",
    "\n",
    "# Calculate variance metrics\n",
    "variance_metrics = calculate_variance_metrics(evaluation_results)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"CLIENT HETEROGENEITY ANALYSIS (Coefficient of Variation)\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "variance_summary = []\n",
    "\n",
    "for aggregator in sorted(variance_metrics.keys()):\n",
    "    print(f\"\\n{aggregator}:\")\n",
    "    for comm_round in sorted(variance_metrics[aggregator].keys(), \n",
    "                             key=lambda x: (x == 'Best', int(x) if isinstance(x, int) else float('inf'))):\n",
    "        metrics = variance_metrics[aggregator][comm_round]\n",
    "        print(f\"  Round {comm_round:>4}: \"\n",
    "              f\"Mean mIoU = {metrics['mean']:.4f}, \"\n",
    "              f\"Std = {metrics['std']:.4f}, \"\n",
    "              f\"CoV = {metrics['cov']:.2f}% \"\n",
    "              f\"({metrics['num_clients']} clients)\")\n",
    "        \n",
    "        variance_summary.append({\n",
    "            'Partition': PARTITION_ID,\n",
    "            'Aggregator': aggregator,\n",
    "            'Communication_Round': comm_round,\n",
    "            'Mean_mIoU': metrics['mean'],\n",
    "            'Std_mIoU': metrics['std'],\n",
    "            'CoV_mIoU': metrics['cov'],\n",
    "            'Num_Clients': metrics['num_clients']\n",
    "        })\n",
    "\n",
    "print(\"\\n✓ Variance metrics calculated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7881fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot variance trends across communication rounds\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Generating Variance Trend Plots\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "for aggregator in sorted(variance_metrics.keys()):\n",
    "    metrics_by_round = variance_metrics[aggregator]\n",
    "    \n",
    "    # Extract data, excluding 'Best' for numeric sorting\n",
    "    numeric_rounds = []\n",
    "    means = []\n",
    "    covs = []\n",
    "    \n",
    "    for comm_round in sorted(metrics_by_round.keys(), \n",
    "                             key=lambda x: (x == 'Best', int(x) if isinstance(x, int) else float('inf'))):\n",
    "        if comm_round != 'Best':\n",
    "            numeric_rounds.append(int(comm_round))\n",
    "            means.append(metrics_by_round[comm_round]['mean'])\n",
    "            covs.append(metrics_by_round[comm_round]['cov'])\n",
    "    \n",
    "    if numeric_rounds:\n",
    "        fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n",
    "        \n",
    "        # Plot 1: Mean mIoU trend\n",
    "        axes[0].plot(numeric_rounds, means, marker='o', linewidth=2, markersize=8, label=aggregator)\n",
    "        axes[0].fill_between(numeric_rounds, means, alpha=0.3)\n",
    "        axes[0].set_xlabel('Communication Round', fontsize=11, fontweight='bold')\n",
    "        axes[0].set_ylabel('Mean mIoU', fontsize=11, fontweight='bold')\n",
    "        axes[0].set_title(f'Mean mIoU Progression - {aggregator}\\nPartition {PARTITION_ID}', \n",
    "                         fontsize=12, fontweight='bold')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        axes[0].legend()\n",
    "        \n",
    "        # Plot 2: Coefficient of Variation trend\n",
    "        axes[1].plot(numeric_rounds, covs, marker='s', linewidth=2, markersize=8, color='red', label='CoV (%)')\n",
    "        axes[1].fill_between(numeric_rounds, covs, alpha=0.3, color='red')\n",
    "        axes[1].set_xlabel('Communication Round', fontsize=11, fontweight='bold')\n",
    "        axes[1].set_ylabel('Coefficient of Variation (%)', fontsize=11, fontweight='bold')\n",
    "        axes[1].set_title(f'Client Heterogeneity (CoV) Over Communication Rounds - {aggregator}', \n",
    "                         fontsize=12, fontweight='bold')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        axes[1].legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'variance_trends_partition_{PARTITION_ID}_{aggregator}.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(f\"  ✓ Saved: variance_trends_partition_{PARTITION_ID}_{aggregator}.png\")\n",
    "\n",
    "print(\"✓ Variance trend plots generated!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab103756",
   "metadata": {},
   "source": [
    "## Section 8: Per-Class Performance Analysis - Label Skew Detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb977980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cityscapes class names (19 classes)\n",
    "CITYSCAPES_CLASSES = [\n",
    "    'road', 'sidewalk', 'building', 'wall', 'fence',\n",
    "    'pole', 'traffic light', 'traffic sign', 'vegetation', 'terrain',\n",
    "    'sky', 'person', 'rider', 'car', 'truck',\n",
    "    'bus', 'train', 'motorcycle', 'bicycle'\n",
    "]\n",
    "\n",
    "def compute_per_class_iou(evaluation_results, aggregator, comm_round):\n",
    "    \"\"\"\n",
    "    Compute per-class IoU for each client at a specific round.\n",
    "    Uses pre-computed confusion matrices from evaluation results.\n",
    "    \n",
    "    Returns:\n",
    "        per_class_data: {client_id: [iou_class_0, iou_class_1, ...]}\n",
    "    \"\"\"\n",
    "    per_class_data = {}\n",
    "    \n",
    "    if aggregator not in evaluation_results:\n",
    "        return per_class_data\n",
    "    \n",
    "    agg_data = evaluation_results[aggregator]\n",
    "    \n",
    "    for client_id in agg_data:\n",
    "        if comm_round not in agg_data[client_id]:\n",
    "            continue\n",
    "        \n",
    "        client_result = agg_data[client_id][comm_round]\n",
    "        \n",
    "        # Extract confusion matrix (can be stored as dict with 'confusion_matrix' key or directly)\n",
    "        if isinstance(client_result, dict) and 'confusion_matrix' in client_result:\n",
    "            client_cm = client_result['confusion_matrix']\n",
    "        elif isinstance(client_result, dict) and 'metrics' in client_result:\n",
    "            # If only metrics are stored, skip\n",
    "            continue\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        # Compute per-class IoU from confusion matrix\n",
    "        per_class_iou = []\n",
    "        for class_id in range(NUM_CLASSES):\n",
    "            tp = client_cm[class_id, class_id]\n",
    "            fp = np.sum(client_cm[:, class_id]) - tp\n",
    "            fn = np.sum(client_cm[class_id, :]) - tp\n",
    "            \n",
    "            iou = tp / (tp + fp + fn) if (tp + fp + fn) > 0 else 0\n",
    "            per_class_iou.append(iou)\n",
    "        \n",
    "        per_class_data[client_id] = per_class_iou\n",
    "    \n",
    "    return per_class_data\n",
    "\n",
    "\n",
    "def create_per_class_heatmap(per_class_data, aggregator, comm_round, partition_id, output_dir=None):\n",
    "    \"\"\"Create and save per-class IoU heatmap for a given aggregator/round.\n",
    "    \n",
    "    Heatmap Values: Per-Class IoU (Intersection over Union)\n",
    "    - Blue (high): Client learned that class well (high IoU)\n",
    "    - Red (low): Client poorly learned class or missing in data (low IoU, label skew)\n",
    "    \"\"\"\n",
    "    if not per_class_data:\n",
    "        return\n",
    "    \n",
    "    # Sort clients by ID for consistent visualization\n",
    "    client_ids = sorted(per_class_data.keys())\n",
    "    \n",
    "    # Create heatmap data: rows=clients, columns=classes\n",
    "    heatmap_matrix = np.array([per_class_data[cid] for cid in client_ids])\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(16, 6))\n",
    "    \n",
    "    # Plot heatmap with blue-red colormap (blue=high IoU, red=low IoU)\n",
    "    im = ax.imshow(heatmap_matrix, cmap='coolwarm_r', aspect='auto', vmin=0, vmax=1)\n",
    "    \n",
    "    # Set labels\n",
    "    ax.set_yticks(np.arange(len(client_ids)))\n",
    "    ax.set_xticks(np.arange(NUM_CLASSES))\n",
    "    ax.set_yticklabels([f'Client {cid}' for cid in client_ids])\n",
    "    ax.set_xticklabels(CITYSCAPES_CLASSES, rotation=45, ha='right')\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(im, ax=ax)\n",
    "    cbar.set_label('Per-Class IoU', rotation=270, labelpad=20)\n",
    "    \n",
    "    # Annotate with IoU values\n",
    "    for i in range(len(client_ids)):\n",
    "        for j in range(NUM_CLASSES):\n",
    "            text = ax.text(j, i, f'{heatmap_matrix[i, j]:.2f}',\n",
    "                          ha=\"center\", va=\"center\", color=\"black\", fontsize=8)\n",
    "    \n",
    "    ax.set_title(f'Per-Class IoU Heatmap - {aggregator} - Round {comm_round}\\n(Blue=High IoU/Well-learned, Red=Low IoU/Label Skew)')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure\n",
    "    if output_dir is None:\n",
    "        output_dir = Path.cwd()\n",
    "    else:\n",
    "        output_dir = Path(output_dir)\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    fig_path = output_dir / f'per_class_iou_heatmap_partition_{partition_id}_{aggregator}_round_{comm_round}.png'\n",
    "    plt.savefig(fig_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"✓ Saved heatmap: {fig_path.name}\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223a35a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process per-class metrics for all evaluated rounds\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"Generating Per-Class IoU Heatmaps\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "per_class_data_all = defaultdict(dict)\n",
    "\n",
    "for aggregator in sorted(evaluation_results.keys()):\n",
    "    print(f\"\\n{aggregator}:\")\n",
    "    \n",
    "    # Get all available rounds for this aggregator\n",
    "    all_rounds = set()\n",
    "    for client_id in evaluation_results[aggregator]:\n",
    "        all_rounds.update(evaluation_results[aggregator][client_id].keys())\n",
    "    \n",
    "    for comm_round in sorted(all_rounds, key=lambda x: (x != \"Best\", x if isinstance(x, str) else int(x))):\n",
    "        # Compute per-class IoU for this round\n",
    "        per_class_data = compute_per_class_iou(evaluation_results, aggregator, comm_round)\n",
    "        \n",
    "        if per_class_data:\n",
    "            # Store for CSV export\n",
    "            per_class_data_all[aggregator][comm_round] = per_class_data\n",
    "            \n",
    "            # Create and save heatmap\n",
    "            create_per_class_heatmap(per_class_data, aggregator, comm_round, PARTITION_ID)\n",
    "            print(f\"  ✓ Round {comm_round}: Generated heatmap for {len(per_class_data)} clients\")\n",
    "\n",
    "print(\"\\n✓ Per-class heatmaps generated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0756f08",
   "metadata": {},
   "source": [
    "## Section 9: Export All Results (Including Variance and Per-Class Metrics) to CSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df42b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_results_to_csv(evaluation_results, variance_summary, per_class_data_all, output_dir: str = '.'):\n",
    "    \"\"\"\n",
    "    Export evaluation results, variance metrics, and per-class IoU to CSV files.\n",
    "    \"\"\"\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    # ===== 1. MAIN EVALUATION RESULTS =====\n",
    "    all_results = []\n",
    "    \n",
    "    for aggregator in evaluation_results:\n",
    "        for client_id in evaluation_results[aggregator]:\n",
    "            for comm_round in evaluation_results[aggregator][client_id]:\n",
    "                result = evaluation_results[aggregator][client_id][comm_round]\n",
    "                \n",
    "                # Handle new format: {'metrics': {...}, 'confusion_matrix': ...}\n",
    "                if isinstance(result, dict) and 'metrics' in result:\n",
    "                    metrics = result['metrics']\n",
    "                else:\n",
    "                    # Handle old format: direct metrics dict\n",
    "                    metrics = result\n",
    "\n",
    "                all_results.append({\n",
    "                    'Partition': PARTITION_ID,\n",
    "                    'Aggregator': aggregator,\n",
    "                    'Client': client_id,\n",
    "                    'Communication_Round': comm_round,\n",
    "                    'mIoU': metrics.get('mIoU', None),\n",
    "                    'F1_Score': metrics.get('F1_Score', None),\n",
    "                    'Accuracy': metrics.get('Accuracy', None),\n",
    "                    'Precision': metrics.get('Precision', None),\n",
    "                    'Recall': metrics.get('Recall', None),\n",
    "                    'fw_mIoU': metrics.get('fw_mIoU', None),\n",
    "                })\n",
    "    \n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    csv_path = output_path / 'evaluation_results.csv'\n",
    "    results_df.to_csv(csv_path, index=False)\n",
    "    print(f\"✓ Results exported to: {csv_path}\")\n",
    "    \n",
    "    # ===== 2. VARIANCE / CLIENT HETEROGENEITY =====\n",
    "    if variance_summary:\n",
    "        variance_df = pd.DataFrame(variance_summary)\n",
    "        variance_path = output_path / 'variance_analysis.csv'\n",
    "        variance_df.to_csv(variance_path, index=False)\n",
    "        print(f\"✓ Variance analysis exported to: {variance_path}\")\n",
    "    \n",
    "    # ===== 3. PER-CLASS IoU HEATMAP DATA =====\n",
    "    per_class_flat = []\n",
    "    \n",
    "    for aggregator in per_class_data_all:\n",
    "        for comm_round in per_class_data_all[aggregator]:\n",
    "            per_class_data = per_class_data_all[aggregator][comm_round]\n",
    "            \n",
    "            for client_id in per_class_data:\n",
    "                class_ious = per_class_data[client_id]\n",
    "                \n",
    "                row = {\n",
    "                    'Partition': PARTITION_ID,\n",
    "                    'Aggregator': aggregator,\n",
    "                    'Client': client_id,\n",
    "                    'Communication_Round': comm_round,\n",
    "                }\n",
    "                \n",
    "                # Add per-class IoU as separate columns\n",
    "                for class_idx, class_name in enumerate(CITYSCAPES_CLASSES):\n",
    "                    row[f'IoU_{class_name}'] = class_ious[class_idx]\n",
    "                \n",
    "                per_class_flat.append(row)\n",
    "    \n",
    "    if per_class_flat:\n",
    "        per_class_df = pd.DataFrame(per_class_flat)\n",
    "        per_class_path = output_path / 'per_class_iou.csv'\n",
    "        per_class_df.to_csv(per_class_path, index=False)\n",
    "        print(f\"✓ Per-class IoU data exported to: {per_class_path}\")\n",
    "    \n",
    "    # ===== 4. SUMMARY STATISTICS =====\n",
    "    summary_df = results_df.groupby(['Partition', 'Aggregator', 'Client']).agg({\n",
    "        'mIoU': ['mean', 'std', 'max'],\n",
    "        'F1_Score': ['mean', 'std', 'max']\n",
    "    }).round(4)\n",
    "    \n",
    "    summary_path = output_path / 'evaluation_summary.csv'\n",
    "    summary_df.to_csv(summary_path)\n",
    "    print(f\"✓ Summary statistics exported to: {summary_path}\")\n",
    "    \n",
    "    return results_df, variance_df if variance_summary else None, per_class_df if per_class_flat else None, summary_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62909fd9",
   "metadata": {},
   "source": [
    "## Analysis Summary and Interpretation Guide\n",
    "\n",
    "### 📊 What Each Analysis Reveals:\n",
    "\n",
    "**1. Variance Analysis (variance_analysis.csv + variance_trends_*.png)**\n",
    "   - **Metric**: Coefficient of Variation (CoV) = (Std / Mean) × 100\n",
    "   - **What it shows**: How heterogeneous (unequal) client performance is at each communication round\n",
    "   - **Interpretation**: \n",
    "     - High CoV (>20%) = Clients are performing very differently → strong non-IIDness or label skew\n",
    "     - Low CoV (<10%) = Clients performing similarly → homogeneous data or good aggregator convergence\n",
    "     - Decreasing CoV over rounds = Aggregator successfully reducing client heterogeneity (good!)\n",
    "     - Increasing CoV = Non-IIDness getting worse (possible issue with aggregator)\n",
    "\n",
    "**2. Per-Class IoU Heatmaps (per_class_iou_heatmap_*.png + per_class_iou.csv)**\n",
    "   - **Rows**: Clients (C0, C1, C2, ...)\n",
    "   - **Columns**: Cityscapes classes (road, car, person, bicycle, etc.)\n",
    "   - **Values**: Per-class IoU (0-1)\n",
    "   - **What it reveals**:\n",
    "     - **Bright green cells**: Class well-learned by that client\n",
    "     - **Red cells**: Class poorly-learned or missing in that client's data\n",
    "   - **Key insights**:\n",
    "     - If a column is red for all clients → that class is rare across all partitions\n",
    "     - If a column is red only for Client A → label skew: that class is underrepresented in Client A's data\n",
    "     - Different \"winning\" clients per class → indicates clear label skew across partitions\n",
    "     - This explains why clients diverge: they're literally learning different object distributions!\n",
    "\n",
    "**Example scenario**:\n",
    "- Client 0: Excellent on \"car\" (0.92 IoU), poor on \"bicycle\" (0.15 IoU)\n",
    "- Client 1: Excellent on \"bicycle\" (0.88 IoU), poor on \"car\" (0.30 IoU)\n",
    "→ Interpretation: City 0 has cars but few bicycles; City 1 has bicycles but few cars. This is natural Non-IID data due to geographic differences!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
